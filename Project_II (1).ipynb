{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a253a873-d16a-4a83-a3da-ce65ea9550cc",
   "metadata": {},
   "source": [
    "pip install tensorflow opencv-python numpy matplotlib tensorflow_hub"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19be1771-5e96-4d96-b12a-fc663fba6aa7",
   "metadata": {},
   "source": [
    "pip install flask"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b53183d6-917b-40f1-9c6f-d9673126771d",
   "metadata": {},
   "source": [
    "pip install speech_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35310e7-2bdc-4501-a53b-b21c5ae17179",
   "metadata": {},
   "source": [
    "# Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60dfe1e7-c80a-4d2e-8548-489ca61fb503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dpree\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "import google.generativeai as genai\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.cluster import KMeans\n",
    "from flask import Flask, render_template\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbddc6dc-3e05-48ca-8b1c-252ab56b23a7",
   "metadata": {},
   "source": [
    "# Outfit Suggestion : Age, Gender => Voice,Message Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32bcb0a6-4b11-495e-8c7d-2fc7a348e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfit_rules = {\n",
    "    \"Kids (Under 3)\": {\n",
    "        \"Male\": [(\"Soft Onesie\", \"Booties\"), (\"T-shirt\", \"Diaper Cover\"), (\"Sweatshirt\", \"Joggers\")],\n",
    "        \"Female\": [(\"Cute Dress\", \"Soft Shoes\"), (\"T-shirt\", \"Leggings\"), (\"Sweatshirt\", \"Joggers\")],\n",
    "        \"Unisex\": [(\"Cotton Romper\", \"Socks\"), (\"Comfy Pajamas\", \"Soft Shoes\"), (\"Sweatshirt\", \"Joggers\")]\n",
    "    },\n",
    "    \"Below 13\": {\n",
    "        \"Male\": [(\"Cartoon T-shirt\", \"Shorts\"), (\"Polo Shirt\", \"Jeans\"), (\"Hoodie\", \"Joggers\")],\n",
    "        \"Female\": [(\"Floral Top\", \"Capris\"), (\"T-shirt\", \"Denim Skirt\"), (\"Sweatshirt\", \"Leggings\")],\n",
    "        \"Unisex\": [(\"Graphic Tee\", \"Jeans\"), (\"Sweatshirt\", \"Joggers\"), (\"Oversized Hoodie\", \"Shorts\")]\n",
    "    },\n",
    "    \"Below 16\": {\n",
    "        \"Male\": [(\"Graphic Tee\", \"Jeans\"), (\"Casual Shirt\", \"Chinos\"), (\"Hoodie\", \"Sweatpants\")],\n",
    "        \"Female\": [(\"Crop Top\", \"Denim Jeans\"), (\"Casual Dress\", \"Sneakers\"), (\"Sweatshirt\", \"Leggings\")],\n",
    "        \"Unisex\": [(\"Oversized Hoodie\", \"Sweatpants\"), (\"Denim Jacket\", \"Jeans\"), (\"Bomber Jacket\", \"Joggers\")]\n",
    "    },\n",
    "    \"16-18\": {\n",
    "        \"Male\": [(\"Slim Fit Tee\", \"Joggers\"), (\"Bomber Jacket\", \"Denim\"), (\"Casual Blazer\", \"Chinos\")],\n",
    "        \"Female\": [(\"Off-shoulder Top\", \"Skinny Jeans\"), (\"Jumpsuit\", \"Heels\"), (\"Oversized Hoodie\", \"Shorts\")],\n",
    "        \"Unisex\": [(\"Graphic Hoodie\", \"Cargo Pants\"), (\"Bomber Jacket\", \"Joggers\"), (\"Loose Shirt\", \"Denim Jeans\")]\n",
    "    },\n",
    "    \"18-30\": {\n",
    "        \"Male\": [(\"Smart Polo\", \"Chinos\"), (\"Casual Blazer\", \"Jeans\"), (\"Denim Jacket\", \"Joggers\")],\n",
    "        \"Female\": [(\"Formal Blouse\", \"Pencil Skirt\"), (\"Casual Shirt\", \"Mom Jeans\"), (\"Cocktail Dress\", \"Heels\")],\n",
    "        \"Unisex\": [(\"Turtleneck\", \"Wide-leg Pants\"), (\"Trench Coat\", \"Black Jeans\"), (\"Oversized Sweater\", \"Cargo Pants\")]\n",
    "    },\n",
    "    \"30-45\": {\n",
    "        \"Male\": [(\"Business Suit\", \"Dress Shoes\"), (\"Turtleneck\", \"Blazer & Trousers\"), (\"Casual Blazer\", \"Chinos\")],\n",
    "        \"Female\": [(\"Elegant Blouse\", \"A-Line Skirt\"), (\"Formal Dress\", \"Heels\"), (\"Chic Coat\", \"Jeans & Boots\")],\n",
    "        \"Unisex\": [(\"Long Coat\", \"Dress Pants\"), (\"Turtleneck\", \"Blazer\"), (\"Smart Casual Sweater\", \"Tailored Pants\")]\n",
    "    },\n",
    "    \"Above 45\": {\n",
    "        \"Male\": [(\"Classic Suit\", \"Leather Shoes\"), (\"Cardigan\", \"Cotton Trousers\"), (\"Formal Shirt\", \"Dress Pants\")],\n",
    "        \"Female\": [(\"Classy Blouse\", \"Tailored Pants\"), (\"Midi Dress\", \"Comfortable Heels\"), (\"Warm Sweater\", \"Leggings\")],\n",
    "        \"Unisex\": [(\"Warm Coat\", \"Soft Trousers\"), (\"Cashmere Sweater\", \"Jeans\"), (\"Turtleneck\", \"Blazer\")]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "463b25bf-e4a1-4cea-8862-2285c52d79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_outfit(age, gender):\n",
    "    \"\"\"Suggests an outfit based on Age & Gender\"\"\"\n",
    "    if age < 3:\n",
    "        category = \"Kids (Under 3)\"\n",
    "    elif age < 13:\n",
    "        category = \"Below 13\"\n",
    "    elif age < 16:\n",
    "        category = \"Below 16\"\n",
    "    elif age < 18:\n",
    "        category = \"16-18\"\n",
    "    elif age < 30:\n",
    "        category = \"18-30\"\n",
    "    elif age < 45:\n",
    "        category = \"30-45\"\n",
    "    else:\n",
    "        category = \"Above 45\"\n",
    "\n",
    "    gender_options = [\"Male\", \"Female\", \"Unisex\"]\n",
    "    \n",
    "    if gender not in gender_options:\n",
    "        return \"❌ Invalid Gender! Please enter Male, Female, or Unisex.\"\n",
    "\n",
    "    return random.choice(outfit_rules[category][gender])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c1fa558-55f8-449e-99c2-5280dd1d45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_voice_input(prompt):\n",
    "    \"\"\"Captures voice input & converts it to text\"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(f\"🎤 {prompt}\")\n",
    "        try:\n",
    "            audio = recognizer.listen(source)\n",
    "            text = recognizer.recognize_google(audio)\n",
    "            return text.capitalize()\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"❌ Could not understand. Try again!\")\n",
    "            return None\n",
    "        except sr.RequestError:\n",
    "            print(\"❌ Speech service unavailable.\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fb9b7ae-43ed-4977-8bb7-5f20c8581f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 How would you like to provide input?\n",
      "1️⃣ Type\n",
      "2️⃣ Voice\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1 for Type, 2 for Voice):  1\n",
      "\n",
      "📌 Enter your age:  21\n",
      "\n",
      "📌 Enter your gender (Male/Female/Unisex):  Female\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Suggested Outfit for a 21-year-old Female: Casual Shirt with Mom Jeans\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"\\n🔹 How would you like to provide input?\")\n",
    "    print(\"1️⃣ Type\")\n",
    "    print(\"2️⃣ Voice\")\n",
    "\n",
    "    choice = input(\"Enter your choice (1 for Type, 2 for Voice): \").strip()\n",
    "\n",
    "    if choice == \"1\":\n",
    "        age = int(input(\"\\n📌 Enter your age: \"))\n",
    "        gender = input(\"\\n📌 Enter your gender (Male/Female/Unisex): \").capitalize()\n",
    "    elif choice == \"2\":\n",
    "        try:\n",
    "            age = int(get_voice_input(\"Say your age...\"))\n",
    "        except ValueError:\n",
    "            print(\"❌ Invalid age input. Switching to manual input.\")\n",
    "            age = int(input(\"\\n📌 Enter your age: \"))\n",
    "\n",
    "        gender = get_voice_input(\"Say your gender (Male, Female, or Unisex)...\")\n",
    "        if not gender:\n",
    "            print(\"⚠️ Switching to manual gender input...\")\n",
    "            gender = input(\"\\n📌 Enter your gender manually: \").capitalize()\n",
    "    else:\n",
    "        print(\"❌ Invalid choice. Please enter 1 or 2.\")\n",
    "        return\n",
    "\n",
    "    # Suggest Outfit\n",
    "    outfit = suggest_outfit(age, gender)\n",
    "\n",
    "    if isinstance(outfit, tuple):\n",
    "        print(f\"\\n🎯 Suggested Outfit for a {age}-year-old {gender}: {outfit[0]} with {outfit[1]}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ {outfit}\")\n",
    "\n",
    "# Run the program\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeb244ad-5fd1-4eb8-ac70-23f4bfda050a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.875\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         2\n",
      "           1       1.00      0.67      0.80         3\n",
      "           2       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.88         8\n",
      "   macro avg       0.89      0.89      0.87         8\n",
      "weighted avg       0.92      0.88      0.88         8\n",
      "\n",
      "Confusion Matrix:\n",
      " [[2 0 0]\n",
      " [1 2 0]\n",
      " [0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "y_true = [0, 1, 1, 2, 2, 0, 1, 2] \n",
    "y_pred = [0, 1, 0, 2, 2, 0, 1, 2]  \n",
    "\n",
    "# Compute Metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edc0a02-772d-4ad4-9be1-da6a7ae4f33b",
   "metadata": {},
   "source": [
    "# Colour Combination Suggestion => Voice,Message Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e99983c2-af0f-44a4-aaf0-d0ace9fb1290",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_combinations = {\n",
    "    \"White\": [\"Black\", \"Navy Blue\", \"Red\", \"Beige\", \"Grey\", \"Olive Green\", \"Pastel Pink\"],\n",
    "    \"Black\": [\"White\", \"Red\", \"Royal Blue\", \"Emerald Green\", \"Gold\", \"Silver\", \"Beige\"],\n",
    "    \"Grey\": [\"White\", \"Black\", \"Burgundy\", \"Pastel Blue\", \"Navy Blue\", \"Mustard\"],\n",
    "    \"Navy Blue\": [\"White\", \"Beige\", \"Light Blue\", \"Red\", \"Brown\", \"Olive Green\"],\n",
    "    \"Sky Blue\": [\"White\", \"Grey\", \"Beige\", \"Navy Blue\", \"Coral\", \"Yellow\"],\n",
    "    \"Royal Blue\": [\"White\", \"Gold\", \"Black\", \"Red\", \"Grey\", \"Peach\"],\n",
    "    \"Emerald Green\": [\"Gold\", \"White\", \"Black\", \"Beige\", \"Navy\", \"Silver\"],\n",
    "    \"Olive Green\": [\"White\", \"Brown\", \"Black\", \"Mustard\", \"Tan\", \"Beige\"],\n",
    "    \"Mustard\": [\"White\", \"Black\", \"Grey\", \"Olive\", \"Brown\", \"Dark Blue\"],\n",
    "    \"Burgundy\": [\"White\", \"Black\", \"Beige\", \"Grey\", \"Mustard\", \"Olive Green\"],\n",
    "    \"Pastel Pink\": [\"White\", \"Grey\", \"Beige\", \"Light Blue\", \"Mint Green\", \"Gold\"],\n",
    "    \"Lavender\": [\"White\", \"Grey\", \"Navy\", \"Light Pink\", \"Mint Green\", \"Beige\"],\n",
    "}\n",
    "\n",
    "outfit_types = {\n",
    "    \"T-shirt\": [\"Jeans\", \"Shorts\", \"Chinos\"],\n",
    "    \"Dress\": [\"Heels\", \"Flats\", \"Boots\"],\n",
    "    \"Jeans\": [\"T-shirt\", \"Shirt\", \"Hoodie\"],\n",
    "    \"Blazer\": [\"Formal Pants\", \"Trousers\", \"Jeans\"],\n",
    "    \"Sweater\": [\"Jeans\", \"Trousers\", \"Chinos\"],\n",
    "    \"Skirt\": [\"Blouse\", \"Crop Top\", \"Turtleneck\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88ac8c14-62c4-4036-a50e-f5e1fe3dd70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_voice_input(prompt):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(prompt)\n",
    "        print(\"🎤 Listening... Speak now.\")\n",
    "        try:\n",
    "            audio = recognizer.listen(source, timeout=10)\n",
    "            text = recognizer.recognize_google(audio)\n",
    "            print(f\"✅ You said: {text}\\n\")\n",
    "            return text.strip().title()\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"❌ Sorry, I didn't understand. Please try again.\")\n",
    "            return get_voice_input(prompt)\n",
    "        except sr.RequestError:\n",
    "            print(\"❌ Could not process the request. Please try again.\")\n",
    "            return get_voice_input(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f2e14f3-1d99-4a30-8d95-39bc8047027c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 How would you like to enter your choice?\n",
      "1️⃣ Type it\n",
      "2️⃣ Say it (Voice Input)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 1 or 2:  1\n",
      "\n",
      "👗 Are you selecting 'Top' or 'Bottom'? Top\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎨 Available Colors: White, Black, Grey, Navy Blue, Sky Blue, Royal Blue, Emerald Green, Olive Green, Mustard, Burgundy, Pastel Pink, Lavender\n",
      "\n",
      "🔹 How would you like to enter your choice?\n",
      "1️⃣ Type it\n",
      "2️⃣ Say it (Voice Input)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 1 or 2:  White\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Invalid input! Please enter 1 for typing or 2 for voice.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 1 or 2:  1\n",
      "\n",
      "Enter or say the color of your chosen clothing: White\n",
      "\n",
      "You selected White Top. Type 'yes' to confirm:  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 Suggested Outfit: White Top with Black Bottom.\n"
     ]
    }
   ],
   "source": [
    "def get_input(prompt, valid_options):\n",
    "    print(\"\\n🔹 How would you like to enter your choice?\")\n",
    "    print(\"1️⃣ Type it\")\n",
    "    print(\"2️⃣ Say it (Voice Input)\")\n",
    "    \n",
    "    choice = input(\"Enter 1 or 2: \").strip()\n",
    "    \n",
    "    while choice not in [\"1\", \"2\"]:\n",
    "        print(\"❌ Invalid input! Please enter 1 for typing or 2 for voice.\")\n",
    "        choice = input(\"Enter 1 or 2: \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        user_input = input(prompt).strip().title()\n",
    "    else:\n",
    "        user_input = get_voice_input(prompt)\n",
    "    \n",
    "    while user_input not in valid_options:\n",
    "        print(f\"❌ Invalid choice! Please select from {valid_options}.\")\n",
    "        if choice == \"1\":\n",
    "            user_input = input(prompt).strip().title()\n",
    "        else:\n",
    "            user_input = get_voice_input(prompt)\n",
    "\n",
    "    return user_input\n",
    "\n",
    "clothing_type = get_input(\"\\n👗 Are you selecting 'Top' or 'Bottom'?\", [\"Top\", \"Bottom\"])\n",
    "\n",
    "print(\"\\n🎨 Available Colors:\", ', '.join(color_combinations.keys()))\n",
    "color = get_input(\"\\nEnter or say the color of your chosen clothing:\", color_combinations.keys())\n",
    "\n",
    "confirmation = input(f\"\\nYou selected {color} {clothing_type}. Type 'yes' to confirm: \").strip().lower()\n",
    "if confirmation != \"yes\":\n",
    "    print(\"❌ Selection canceled. Please restart the program.\")\n",
    "    exit()\n",
    "\n",
    "matching_colors = color_combinations[color]\n",
    "suggested_color = random.choice(matching_colors)\n",
    "\n",
    "if clothing_type == \"Top\":\n",
    "    print(f\"\\n🎉 Suggested Outfit: {color} Top with {suggested_color} Bottom.\")\n",
    "else:\n",
    "    print(f\"\\n🎉 Suggested Outfit: {suggested_color} Top with {color} Bottom.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b51a5a5-57fa-49de-a94b-43318cf142b9",
   "metadata": {},
   "source": [
    "# Using Camera for Color Detection and Suggestion of Complementary colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841c7e6-b180-4f58-9c8b-43708bf99e67",
   "metadata": {},
   "source": [
    "# Identifying Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1986cea2-419c-44c6-9975-17fad1886fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_ranges = {\n",
    "    \"Black\": (0, 10),\n",
    "    \"White\": (0, 0),\n",
    "    \"Red\": (0, 10),\n",
    "    \"Green\": (35, 85),\n",
    "    \"Blue\": (100, 140),\n",
    "    \"Yellow\": (20, 40),\n",
    "    \"Orange\": (10, 20),\n",
    "    \"Purple\": (130, 160),\n",
    "    \"Pink\": (160, 180),\n",
    "    \"Gray\": (0, 179),  \n",
    "    \"Light Blue\": (85, 100),\n",
    "    \"Light Green\": (55, 75),\n",
    "    \"Teal\": (80, 100),\n",
    "    \"Violet\": (145, 160),\n",
    "    \"Cyan\": (85, 95),\n",
    "    \"Turquoise\": (95, 105),\n",
    "    \"Lime\": (35, 55),\n",
    "    \"Olive\": (40, 50),\n",
    "    \"Magenta\": (145, 160),\n",
    "    \"Brown\": (10, 30),\n",
    "    \"Beige\": (20, 30),\n",
    "    \"Peach\": (10, 20),\n",
    "    \"Maroon\": (0, 10),\n",
    "    \"Gold\": (25, 40),\n",
    "    \"Copper\": (10, 25),\n",
    "    \"Silver\": (0, 180),\n",
    "    \"Indigo\": (130, 150),\n",
    "    \"Burgundy\": (0, 5),\n",
    "    \"Lavender\": (140, 160),\n",
    "    \"Mint\": (45, 60)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ab7e17c-1421-4dad-86d2-586ef513fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_combinations = {\n",
    "    \"White\": [\"Black\", \"Navy Blue\", \"Red\", \"Beige\", \"Grey\", \"Olive Green\", \"Pastel Pink\"],\n",
    "    \"Black\": [\"White\", \"Red\", \"Royal Blue\", \"Emerald Green\", \"Gold\", \"Silver\", \"Beige\"],\n",
    "    \"Grey\": [\"White\", \"Black\", \"Burgundy\", \"Pastel Blue\", \"Navy Blue\", \"Mustard\"],\n",
    "    \"Navy Blue\": [\"White\", \"Beige\", \"Light Blue\", \"Red\", \"Brown\", \"Olive Green\"],\n",
    "    \"Sky Blue\": [\"White\", \"Grey\", \"Beige\", \"Navy Blue\", \"Coral\", \"Yellow\"],\n",
    "    \"Blue\": [\"White\", \"Gold\", \"Black\", \"Red\", \"Grey\", \"Peach\"],\n",
    "    \"Pastel Blue\": [\"White\", \"Beige\", \"Grey\", \"Navy\", \"Blush Pink\", \"Mustard\"],\n",
    "    \"Dark Blue\": [\"White\", \"Tan\", \"Maroon\", \"Light Blue\", \"Olive\", \"Grey\"],\n",
    "    \"Light Green\": [\"White\", \"Navy\", \"Tan\", \"Pastel Yellow\", \"Grey\", \"Black\"],\n",
    "    \"Emerald Green\": [\"Gold\", \"White\", \"Black\", \"Beige\", \"Navy\", \"Silver\"],\n",
    "    \"Olive Green\": [\"White\", \"Brown\", \"Black\", \"Mustard\", \"Tan\", \"Beige\"],\n",
    "    \"Lime Green\": [\"Navy\", \"Black\", \"White\", \"Beige\", \"Pink\", \"Brown\"],\n",
    "    \"Dark Green\": [\"Beige\", \"White\", \"Grey\", \"Black\", \"Mustard\", \"Gold\"],\n",
    "    \"Neon Green\": [\"Black\", \"White\", \"Dark Blue\", \"Yellow\", \"Silver\"],\n",
    "    \"Mint Green\": [\"White\", \"Pastel Pink\", \"Beige\", \"Light Blue\", \"Grey\", \"Gold\"],\n",
    "    \"Teal\": [\"White\", \"Black\", \"Brown\", \"Beige\", \"Yellow\", \"Red\"],\n",
    "    \"Turquoise\": [\"White\", \"Navy\", \"Gold\", \"Brown\", \"Coral\", \"Beige\"],\n",
    "    \"Light Yellow\": [\"White\", \"Grey\", \"Pastel Blue\", \"Black\", \"Olive\", \"Navy\"],\n",
    "    \"Mustard\": [\"White\", \"Black\", \"Grey\", \"Olive\", \"Brown\", \"Dark Blue\"],\n",
    "    \"Bright Yellow\": [\"Black\", \"White\", \"Navy\", \"Red\", \"Grey\", \"Green\"],\n",
    "    \"Gold\": [\"White\", \"Black\", \"Navy\", \"Red\", \"Emerald Green\", \"Beige\"],\n",
    "    \"Beige\": [\"White\", \"Brown\", \"Black\", \"Red\", \"Navy\", \"Olive Green\"],\n",
    "    \"Khaki\": [\"Black\", \"White\", \"Brown\", \"Burgundy\", \"Navy\", \"Mustard\"],\n",
    "    \"Brown\": [\"White\", \"Beige\", \"Black\", \"Olive Green\", \"Mustard\", \"Grey\"],\n",
    "    \"Tan\": [\"White\", \"Black\", \"Olive Green\", \"Navy\", \"Brown\", \"Burgundy\"],\n",
    "    \"Peach\": [\"White\", \"Grey\", \"Beige\", \"Light Blue\", \"Olive\", \"Gold\"],\n",
    "    \"Orange\": [\"White\", \"Black\", \"Grey\", \"Navy\", \"Brown\", \"Olive Green\"],\n",
    "    \"Burnt Orange\": [\"White\", \"Black\", \"Brown\", \"Olive\", \"Gold\", \"Beige\"],\n",
    "    \"Dark Red\": [\"White\", \"Black\", \"Grey\", \"Gold\", \"Brown\", \"Olive\"],\n",
    "    \"Bright Red\": [\"White\", \"Black\", \"Navy\", \"Grey\", \"Beige\", \"Gold\"],\n",
    "    \"Maroon\": [\"White\", \"Black\", \"Beige\", \"Grey\", \"Mustard\", \"Olive Green\"],\n",
    "    \"Burgundy\": [\"White\", \"Black\", \"Grey\", \"Gold\", \"Beige\", \"Olive\"],\n",
    "    \"Pink\": [\"White\", \"Grey\", \"Black\", \"Red\", \"Navy\", \"Gold\"],\n",
    "    \"Pastel Pink\": [\"White\", \"Grey\", \"Beige\", \"Light Blue\", \"Mint Green\", \"Gold\"],\n",
    "    \"Blush Pink\": [\"White\", \"Black\", \"Navy\", \"Beige\", \"Grey\", \"Olive Green\"],\n",
    "    \"Fuchsia\": [\"White\", \"Black\", \"Grey\", \"Navy\", \"Emerald Green\", \"Gold\"],\n",
    "    \"Purple\": [\"White\", \"Black\", \"Grey\", \"Navy\", \"Burgundy\", \"Gold\"],\n",
    "    \"Lavender\": [\"White\", \"Grey\", \"Navy\", \"Light Pink\", \"Mint Green\", \"Beige\"],\n",
    "    \"Violet\": [\"White\", \"Black\", \"Grey\", \"Burgundy\", \"Gold\", \"Teal\"],\n",
    "    \"Silver\": [\"Black\", \"White\", \"Grey\", \"Navy\", \"Burgundy\", \"Gold\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6723c896-9f9c-49a1-8828-0813416abdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_matching_colors(selected_color):\n",
    "    return color_combinations.get(selected_color, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b245c586-0c3b-4341-95b2-ec35682d1039",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "start_time = time.time()\n",
    "fixed_color = None\n",
    "color_locked = False\n",
    "suggestions_displayed = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46e4c41b-79d1-4392-8a8d-0c2ea23fb425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔒 Color locked: Green. Is this correct? (yes/no)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " mo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching colors for Green:\n",
      "Suggested Matching Colors: \n",
      "\n",
      "⏳ 30 seconds up! Closing the camera.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    center_region = frame[h//2-75:h//2+75, w//2-75:w//2+75]\n",
    "\n",
    "    hsv_region = cv2.cvtColor(center_region, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    center_hue = hsv_region[hsv_region.shape[0]//2, hsv_region.shape[1]//2][0]\n",
    "\n",
    "    detected_color = \"Unknown\"\n",
    "    for color, (lower, upper) in hue_ranges.items():\n",
    "        if lower <= center_hue <= upper:\n",
    "            detected_color = color\n",
    "            break\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    if elapsed_time >= 15 and not color_locked:\n",
    "        fixed_color = detected_color\n",
    "        color_locked = True\n",
    "        print(f\"\\n🔒 Color locked: {fixed_color}. Is this correct? (yes/no)\")\n",
    "        user_input = input().strip().lower()\n",
    "\n",
    "        if user_input == \"no\":\n",
    "            print(\"\\n⏳ Re-identifying color for the next 15 seconds...\")\n",
    "            fixed_color = None\n",
    "            color_locked = False\n",
    "            suggestions_displayed = False  \n",
    "            start_time = time.time() \n",
    "\n",
    "    elif color_locked:\n",
    "        detected_color = fixed_color  \n",
    "        if not suggestions_displayed:\n",
    "            matching_colors = suggest_matching_colors(fixed_color)\n",
    "            print(f\"Matching colors for {fixed_color}:\")\n",
    "            print(f\"Suggested Matching Colors: {', '.join(matching_colors)}\")\n",
    "            suggestions_displayed = True  \n",
    "\n",
    "    cv2.rectangle(frame, (w//2-75, h//2-75), (w//2+75, h//2+75), (255, 255, 255), 2)\n",
    "    cv2.putText(frame, f\"Detected: {detected_color}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.imshow(\"Color Detector\", frame)\n",
    "    if elapsed_time >= 30:\n",
    "        print(\"\\n⏳ 30 seconds up! Closing the camera.\")\n",
    "        break\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f34522d-2006-4003-92c1-2923753f6575",
   "metadata": {},
   "source": [
    "# Identifying the Clothing "
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e58b20e-644c-4022-9213-4d92b75b9ac3",
   "metadata": {},
   "source": [
    "# Not Precise in Accuracy\n",
    "\n",
    "model = MobileNetV2(weights='imagenet')  \n",
    "\n",
    "# Define clothing categories\n",
    "clothing_labels = {\n",
    "    \"Top\": [\"shirt\", \"t-shirt\", \"blouse\", \"jacket\", \"hoodie\", \"sweater\"],\n",
    "    \"Bottom\": [\"jeans\", \"pants\", \"shorts\", \"skirt\", \"trousers\"]\n",
    "}\n",
    "\n",
    "# Function to classify clothing\n",
    "def classify_clothing(frame):\n",
    "    img_resized = cv2.resize(frame, (224, 224))\n",
    "    img_array = image.img_to_array(img_resized)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "\n",
    "    # Make prediction\n",
    "    predictions = model.predict(img_array)\n",
    "    decoded_preds = tf.keras.applications.mobilenet_v2.decode_predictions(predictions, top=5)[0]\n",
    "    \n",
    "    predicted_label = decoded_preds[0][1].lower()\n",
    "    print(f\"Detected Item: {predicted_label}\")\n",
    "\n",
    "    # Determine Top or Bottom\n",
    "    for category, items in clothing_labels.items():\n",
    "        if any(item in predicted_label for item in items):\n",
    "            return category, predicted_label\n",
    "\n",
    "    return \"Unknown\", predicted_label\n",
    "\n",
    "# Initialize camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "print(\"📷 Camera activated. Show clothing item...\")\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"❌ Error: Cannot read frame.\")\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)  # Flip for correct orientation\n",
    "\n",
    "    # Show the camera feed\n",
    "    cv2.imshow(\"Clothing Detector\", frame)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Auto capture after 15s\n",
    "    if elapsed_time >= 15:\n",
    "        print(\"📸 Capturing Image...\")\n",
    "        category, item = classify_clothing(frame)\n",
    "        print(f\"📌 Clothing Type: {category} ({item})\")\n",
    "        break\n",
    "\n",
    "    # Auto close after 20s\n",
    "    if elapsed_time >= 20:\n",
    "        print(\"⏳ Time up! Closing the camera.\")\n",
    "        break\n",
    "\n",
    "    # Allow manual exit by pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"🛑 Exiting...\")\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"📴 Camera closed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ecaf2-164c-4119-9108-c5ac1f17a5b9",
   "metadata": {},
   "source": [
    "# Image Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d75eb5-0871-4632-bd0b-41eac4e3a6e4",
   "metadata": {},
   "source": [
    "# Customised CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a25359b5-a9f3-457a-b253-ddcda36be314",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:\\\\Users\\\\dpree\\\\Downloads\\\\images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87d41198-772e-45e2-bf56-9f6cfdf82b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,      \n",
    "    width_shift_range=0.2,  \n",
    "    height_shift_range=0.2, \n",
    "    shear_range=0.2,        \n",
    "    zoom_range=0.2,         \n",
    "    horizontal_flip=True,   \n",
    "    validation_split=0.2    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4012baa-02e7-45bb-8a0b-169438ccb2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35284 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Load Data Using Generators\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1c922ab-203b-4a4d-ac5a-48e0b94c956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8812 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=50,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a4700d2-2e8f-406a-b24e-e8eee1ff0093",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af5d7de1-7443-4dfb-a6ac-382aeabd44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Add Custom Classification Layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  # Reduce feature map size\n",
    "x = Dense(512, activation='relu')(x)  # Fully connected layer\n",
    "x = Dropout(0.3)(x)  # Dropout to prevent overfitting\n",
    "x = BatchNormalization()(x)  # Normalize activations\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(len(train_generator.class_indices), activation='softmax')(x)  # Output layer with 23 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96990ad2-d95f-4d14-88ab-c4eecbda59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29513af2-2225-446f-a81a-55ac636d56db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Learning Rate Scheduling & Early Stopping\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33c4a78c-642c-4144-8dee-05686bb53728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpree\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3243s\u001b[0m 3s/step - accuracy: 0.2800 - loss: 2.4677 - val_accuracy: 0.4761 - val_loss: 1.5988 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4465s\u001b[0m 4s/step - accuracy: 0.4382 - loss: 1.7117 - val_accuracy: 0.4907 - val_loss: 1.4963 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3674s\u001b[0m 3s/step - accuracy: 0.4646 - loss: 1.6019 - val_accuracy: 0.5124 - val_loss: 1.4339 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2976s\u001b[0m 3s/step - accuracy: 0.4901 - loss: 1.5107 - val_accuracy: 0.5228 - val_loss: 1.3908 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3092s\u001b[0m 3s/step - accuracy: 0.5059 - loss: 1.4526 - val_accuracy: 0.5246 - val_loss: 1.3824 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3640s\u001b[0m 3s/step - accuracy: 0.5135 - loss: 1.4193 - val_accuracy: 0.5323 - val_loss: 1.3498 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2927s\u001b[0m 3s/step - accuracy: 0.5255 - loss: 1.3767 - val_accuracy: 0.5267 - val_loss: 1.3556 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30391s\u001b[0m 28s/step - accuracy: 0.5319 - loss: 1.3604 - val_accuracy: 0.5347 - val_loss: 1.3246 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3548s\u001b[0m 3s/step - accuracy: 0.5387 - loss: 1.3412 - val_accuracy: 0.5413 - val_loss: 1.3259 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3783s\u001b[0m 3s/step - accuracy: 0.5391 - loss: 1.3196 - val_accuracy: 0.5424 - val_loss: 1.3099 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3919s\u001b[0m 3s/step - accuracy: 0.5439 - loss: 1.3069 - val_accuracy: 0.5352 - val_loss: 1.3149 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3525s\u001b[0m 3s/step - accuracy: 0.5463 - loss: 1.2932 - val_accuracy: 0.5409 - val_loss: 1.3067 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4549s\u001b[0m 4s/step - accuracy: 0.5531 - loss: 1.2689 - val_accuracy: 0.5468 - val_loss: 1.2982 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3050s\u001b[0m 3s/step - accuracy: 0.5595 - loss: 1.2585 - val_accuracy: 0.5445 - val_loss: 1.2907 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3202s\u001b[0m 3s/step - accuracy: 0.5605 - loss: 1.2502 - val_accuracy: 0.5534 - val_loss: 1.2701 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3530s\u001b[0m 3s/step - accuracy: 0.5664 - loss: 1.2445 - val_accuracy: 0.5438 - val_loss: 1.2875 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3288s\u001b[0m 3s/step - accuracy: 0.5662 - loss: 1.2315 - val_accuracy: 0.5519 - val_loss: 1.2777 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5756 - loss: 1.2088\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3007s\u001b[0m 3s/step - accuracy: 0.5756 - loss: 1.2088 - val_accuracy: 0.5569 - val_loss: 1.2720 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3502s\u001b[0m 3s/step - accuracy: 0.5711 - loss: 1.2175 - val_accuracy: 0.5582 - val_loss: 1.2541 - learning_rate: 5.0000e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3734s\u001b[0m 3s/step - accuracy: 0.5807 - loss: 1.2006 - val_accuracy: 0.5548 - val_loss: 1.2756 - learning_rate: 5.0000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=20, \n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")\n",
    "model.save(\"fashion_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f11e4a2-6cfd-4c11-8537-4c8bedc7f9ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training Samples: 35284\n",
      "✅ Validation Samples: 8812\n",
      "✅ Classes: {'Men_Denim': 0, 'Men_Jacket_Vests': 1, 'Men_Pants': 2, 'Men_Shorts': 3, 'Men_Shorts_Polos': 4, 'Men_Suiting': 5, 'Men_Sweaters': 6, 'Men_Sweatshirts': 7, 'Men_Tees_Tank': 8, 'WOmen_Graphic_tees': 9, 'Women_Blouse_Shirts': 10, 'Women_Cardigans': 11, 'Women_Denim': 12, 'Women_Dresses': 13, 'Women_Jacket_Coats': 14, 'Women_Jumpsuits': 15, 'Women_Leggins': 16, 'Women_Pants': 17, 'Women_Shorts': 18, 'Women_Skirts': 19, 'Women_Sweaters': 20, 'Women_Sweatshirts': 21, 'Women_Tops': 22}\n"
     ]
    }
   ],
   "source": [
    "print(f\"✅ Training Samples: {train_generator.samples}\")\n",
    "print(f\"✅ Validation Samples: {validation_generator.samples}\")\n",
    "print(f\"✅ Classes: {train_generator.class_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baae9b49-8db9-4823-bfb0-fd44b758a7cd",
   "metadata": {},
   "source": [
    "# CNN Model with Data Generators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b927658b-e5c3-43d4-9864-a87616dd510b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1102/1102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1753s\u001b[0m 2s/step - accuracy: 0.2688 - loss: 2.3934 - val_accuracy: 0.4074 - val_loss: 1.8530\n",
      "Epoch 2/10\n",
      "\u001b[1m   1/1102\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19:15\u001b[0m 1s/step - accuracy: 0.4062 - loss: 1.8567"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpree\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1102/1102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257us/step - accuracy: 0.4062 - loss: 1.8567 - val_accuracy: 0.4167 - val_loss: 2.0067\n",
      "Epoch 3/10\n",
      "\u001b[1m1102/1102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1811s\u001b[0m 2s/step - accuracy: 0.3917 - loss: 1.8745 - val_accuracy: 0.4411 - val_loss: 1.6900\n",
      "Epoch 4/10\n",
      "\u001b[1m1102/1102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 280us/step - accuracy: 0.4062 - loss: 1.8030 - val_accuracy: 0.6667 - val_loss: 1.2151\n",
      "Epoch 5/10\n",
      "\u001b[1m1102/1102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1774s\u001b[0m 2s/step - accuracy: 0.4474 - loss: 1.6562 - val_accuracy: 0.4628 - val_loss: 1.5750\n",
      "Epoch 6/10\n",
      "\u001b[1m1102/1102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 170us/step - accuracy: 0.4375 - loss: 1.4256 - val_accuracy: 0.3333 - val_loss: 2.1124\n",
      "Epoch 7/10\n",
      "\u001b[1m1102/1102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2618s\u001b[0m 2s/step - accuracy: 0.4866 - loss: 1.5121 - val_accuracy: 0.4888 - val_loss: 1.5123\n",
      "Epoch 8/10\n",
      "\u001b[1m1102/1102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 546us/step - accuracy: 0.5312 - loss: 1.4652 - val_accuracy: 0.5000 - val_loss: 1.5850\n",
      "Epoch 9/10\n",
      "\u001b[1m1102/1102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1774s\u001b[0m 2s/step - accuracy: 0.5299 - loss: 1.3878 - val_accuracy: 0.4968 - val_loss: 1.4647\n",
      "Epoch 10/10\n",
      "\u001b[1m1102/1102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 275us/step - accuracy: 0.7188 - loss: 1.0366 - val_accuracy: 0.5000 - val_loss: 1.3652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved as fashion_classifier_generator.h5\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_generator.num_classes, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,  # Steps per epoch\n",
    "    epochs=10,  # Train for 10 epochs\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,  # Validation steps\n",
    ")\n",
    "\n",
    "model.save(\"fashion_classifier_generator.h5\")\n",
    "print(\"✅ Model saved as fashion_classifier_generator.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26f5a0-9828-4263-b555-e50408e5aa8c",
   "metadata": {},
   "source": [
    "# Outfit Identifying and Suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b2f706-aa72-4e65-98f4-794c00ef0374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"fashion_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00bda241-11cd-412e-8811-03ca1204e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\n",
    "    \"Men_Denim\", \"Men_Jacket_Vests\", \"Men_Pants\", \"Men_Shorts\", \"Men_Shorts_Polos\", \n",
    "    \"Men_Suiting\", \"Men_Sweaters\", \"Men_Sweatshirts\", \"Men_Tees_Tank\", \"WOmen_Graphic_tees\", \n",
    "    \"Women_Blouse_Shirts\", \"Women_Cardigans\", \"Women_Denim\", \"Women_Dresses\", \"Women_Jacket_Coats\",\n",
    "    \"Women_Jumpsuits\", \"Women_Leggins\", \"Women_Pants\", \"Women_Shorts\", \"Women_Skirts\", \n",
    "    \"Women_Sweaters\", \"Women_Sweatshirts\", \"Women_Tops\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d4149f4-b1a3-41c0-846e-02c8c4090e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfit_suggestions = {\n",
    "    \"Men_Denim\": [\"T-shirt\", \"Sweatshirt\", \"Sneakers\"],\n",
    "    \"Men_Jacket_Vests\": [\"Shirt\", \"Jeans\", \"Boots\"],\n",
    "    \"Men_Pants\": [\"Shirt\", \"Blazer\", \"Dress Shoes\"],\n",
    "    \"Men_Shorts\": [\"T-shirt\", \"Sneakers\"],\n",
    "    \"Men_Shorts_Polos\": [\"Polo T-shirt\", \"Loafers\"],\n",
    "    \"Men_Suiting\": [\"Dress Shirt\", \"Tie\", \"Formal Shoes\"],\n",
    "    \"Men_Sweaters\": [\"Jeans\", \"Boots\", \"Scarf\"],\n",
    "    \"Men_Sweatshirts\": [\"Joggers\", \"Sneakers\"],\n",
    "    \"Men_Tees_Tank\": [\"Jeans\", \"Shorts\", \"Sneakers\"],\n",
    "    \"WOmen_Graphic_tees\": [\"Denim Shorts\", \"Sneakers\"],\n",
    "    \"Women_Blouse_Shirts\": [\"Skirt\", \"Heels\"],\n",
    "    \"Women_Cardigans\": [\"Jeans\", \"Boots\"],\n",
    "    \"Women_Denim\": [\"Crop Top\", \"Sneakers\"],\n",
    "    \"Women_Dresses\": [\"Heels\", \"Handbag\"],\n",
    "    \"Women_Jacket_Coats\": [\"Jeans\", \"Boots\"],\n",
    "    \"Women_Jumpsuits\": [\"Heels\", \"Sneakers\"],\n",
    "    \"Women_Leggins\": [\"Tunic\", \"Sneakers\"],\n",
    "    \"Women_Pants\": [\"Blouse\", \"Heels\"],\n",
    "    \"Women_Shorts\": [\"Tank Top\", \"Sneakers\"],\n",
    "    \"Women_Skirts\": [\"Blouse\", \"Heels\"],\n",
    "    \"Women_Sweaters\": [\"Jeans\", \"Boots\"],\n",
    "    \"Women_Sweatshirts\": [\"Joggers\", \"Sneakers\"],\n",
    "    \"Women_Tops\": [\"Jeans\", \"Heels\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd439324-d897-499f-930c-a82c7231cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "start_time = time.time()\n",
    "fixed_prediction = None\n",
    "prediction_locked = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fc3f01b-6b8e-4e7d-85e2-3e6795380e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\n",
      "🔒 Clothing detected: Women_Dresses. Is this correct? (yes/no)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳ Re-detecting clothing for the next 15 seconds...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\n",
      "🔒 Clothing detected: Women_Tops. Is this correct? (yes/no)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "\n",
      "👗 Suggested Outfit for Women_Tops: Jeans, Heels\n",
      "\n",
      "⏳ 30 seconds up! Closing the camera.\n"
     ]
    }
   ],
   "source": [
    "def predict_clothing(img):\n",
    "    \"\"\"Preprocess the image and predict clothing type.\"\"\"\n",
    "    img = cv2.resize(img, (224, 224))  # Resize to match model input\n",
    "    img = img / 255.0  # Normalize\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(img)\n",
    "    predicted_class = class_labels[np.argmax(predictions)]\n",
    "    return predicted_class\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Get center 200x200 pixels for clothing detection\n",
    "    h, w, _ = frame.shape\n",
    "    center_region = frame[h//2-100:h//2+100, w//2-100:w//2+100]\n",
    "\n",
    "    detected_type = predict_clothing(center_region)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Lock clothing prediction after 15s\n",
    "    if elapsed_time >= 15 and not prediction_locked:\n",
    "        fixed_prediction = detected_type\n",
    "        prediction_locked = True\n",
    "        print(f\"\\n🔒 Clothing detected: {fixed_prediction}. Is this correct? (yes/no)\")\n",
    "        user_input = input().strip().lower()\n",
    "\n",
    "        if user_input == \"no\":\n",
    "            print(\"\\n⏳ Re-detecting clothing for the next 15 seconds...\")\n",
    "            fixed_prediction = None\n",
    "            prediction_locked = False\n",
    "            start_time = time.time() \n",
    "\n",
    "    elif prediction_locked:\n",
    "        detected_type = fixed_prediction  \n",
    "\n",
    "        # Suggest an outfit only once after locking\n",
    "        if fixed_prediction in outfit_suggestions:\n",
    "            matching_outfit = outfit_suggestions[fixed_prediction]\n",
    "            print(f\"\\n👗 Suggested Outfit for {fixed_prediction}: {', '.join(matching_outfit)}\")\n",
    "\n",
    "    # Display detected clothing type\n",
    "    cv2.putText(frame, f\"Detected: {detected_type}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Clothing Detector\", frame)\n",
    "\n",
    "    # Exit after 30s\n",
    "    if elapsed_time >= 30:\n",
    "        print(\"\\n⏳ 30 seconds up! Closing the camera.\")\n",
    "        break\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release camera\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91adfe69-985b-4750-8f5a-a607e1a3d6b0",
   "metadata": {},
   "source": [
    "# Color+Cloth Combined"
   ]
  },
  {
   "cell_type": "raw",
   "id": "854c9963-49bf-4394-8344-63fdf267529e",
   "metadata": {},
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyCnRvmPOboIvqZg2dINnsUvvHMFoyWruAs\")\n",
    "\n",
    "models = genai.list_models()\n",
    "for model in models:\n",
    "    print(model.name)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "567dda51-9624-4f70-bc06-846e28183abc",
   "metadata": {},
   "source": [
    "model = load_model(\"fashion_classifier.h5\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "667c71fe-74cc-43b1-9b9a-049e4ab9a01d",
   "metadata": {},
   "source": [
    "genai.configure(api_key=\"AIzaSyCnRvmPOboIvqZg2dINnsUvvHMFoyWruAs\") "
   ]
  },
  {
   "cell_type": "raw",
   "id": "90e11ea6-75dd-496f-9899-b1726db73ede",
   "metadata": {},
   "source": [
    "color_ranges = {\n",
    "    \"Black\": (0, 10),\n",
    "    \"White\": (0, 0),\n",
    "    \"Red\": (0, 10),\n",
    "    \"Green\": (35, 85),\n",
    "    \"Blue\": (100, 140),\n",
    "    \"Yellow\": (20, 40),\n",
    "    \"Orange\": (10, 20),\n",
    "    \"Purple\": (130, 160),\n",
    "    \"Pink\": (160, 180),\n",
    "    \"Gray\": (0, 179),  \n",
    "    \"Light Blue\": (85, 100),\n",
    "    \"Light Green\": (55, 75),\n",
    "    \"Teal\": (80, 100),\n",
    "    \"Violet\": (145, 160),\n",
    "    \"Cyan\": (85, 95),\n",
    "    \"Turquoise\": (95, 105),\n",
    "    \"Lime\": (35, 55),\n",
    "    \"Olive\": (40, 50),\n",
    "    \"Magenta\": (145, 160),\n",
    "    \"Brown\": (10, 30),\n",
    "    \"Beige\": (20, 30),\n",
    "    \"Peach\": (10, 20),\n",
    "    \"Maroon\": (0, 10),\n",
    "    \"Gold\": (25, 40),\n",
    "    \"Copper\": (10, 25),\n",
    "    \"Silver\": (0, 180),\n",
    "    \"Indigo\": (130, 150),\n",
    "    \"Burgundy\": (0, 5),\n",
    "    \"Lavender\": (140, 160),\n",
    "    \"Mint\": (45, 60)\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fecaf28d-8d24-481b-8e2c-032c78f7c767",
   "metadata": {},
   "source": [
    "class_labels = [\n",
    "    \"Men_Denim\", \"Men_Jacket_Vests\", \"Men_Pants\", \"Men_Shorts\", \"Men_Shorts_Polos\", \n",
    "    \"Men_Suiting\", \"Men_Sweaters\", \"Men_Sweatshirts\", \"Men_Tees_Tank\", \"WOmen_Graphic_tees\", \n",
    "    \"Women_Blouse_Shirts\", \"Women_Cardigans\", \"Women_Denim\", \"Women_Dresses\", \"Women_Jacket_Coats\",\n",
    "    \"Women_Jumpsuits\", \"Women_Leggins\", \"Women_Pants\", \"Women_Shorts\", \"Women_Skirts\", \n",
    "    \"Women_Sweaters\", \"Women_Sweatshirts\", \"Women_Tops\"\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1de549d3-033d-419a-b100-34847d3aece2",
   "metadata": {},
   "source": [
    "def detect_clothing_color(frame):\n",
    "    \"\"\"\n",
    "    Detects the dominant clothing color using HSV hue ranges.\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    detected_color = \"unknown\"\n",
    "\n",
    "    for color, (lower, upper) in color_ranges.items():\n",
    "        mask = cv2.inRange(hsv, np.array(lower), np.array(upper))\n",
    "        if np.sum(mask) > 5000:  # If sufficient pixels match the color\n",
    "            detected_color = color\n",
    "            break\n",
    "\n",
    "    return detected_color"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64cf27b1-eff9-4da1-8767-0e4199d21bc7",
   "metadata": {},
   "source": [
    "def predict_clothing_type(frame):\n",
    "    \"\"\"\n",
    "    Predicts clothing type using the fashion_classifier.h5 model.\n",
    "    \"\"\"\n",
    "    # Resize and preprocess the image\n",
    "    img = cv2.resize(frame, (128, 128))  # Adjust based on model input size\n",
    "    img = img_to_array(img) / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    # Predict clothing type\n",
    "    predictions = model.predict(img)\n",
    "    class_index = np.argmax(predictions)\n",
    "    return class_labels[class_index]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bafa971c-8a57-4e16-9e4e-18e5bc899e07",
   "metadata": {},
   "source": [
    "def get_gemini_suggestion(color, clothing_type):\n",
    "    \"\"\"\n",
    "    Generate a compatible outfit suggestion using Gemini.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "    prompt = f\"I am wearing a {color} {clothing_type}. Suggest a compatible outfit combination.\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text if response else \"No suggestion available.\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "b89273fe-28b1-4060-8f16-2c5866b34770",
   "metadata": {},
   "source": [
    "def generate_outfit_image(outfit_description):\n",
    "    \"\"\"\n",
    "    Generate an outfit image based on the suggestion using Gemini's image model.\n",
    "    \"\"\"\n",
    "    image_model = genai.GenerativeModel(\"imagen-3.0-generate-002\")\n",
    "    prompt = f\"Generate an image of an outfit consisting of {outfit_description}.\"\n",
    "    \n",
    "    image_response = image_model.generate_content(prompt)\n",
    "    return image_response.text if image_response else \"No image available.\"\n",
    "\n",
    "def display_generated_image(image_url):\n",
    "    \"\"\"\n",
    "    Fetch and display the generated outfit image.\n",
    "    \"\"\"\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        img.show()\n",
    "    else:\n",
    "        print(\"Failed to fetch the image.\")\n",
    "\n",
    "# 🎥 Initialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Press 'q' to capture clothing and exit...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Detect clothing color\n",
    "    detected_color = detect_clothing_color(frame)\n",
    "\n",
    "    # Predict clothing type\n",
    "    clothing_type = predict_clothing_type(frame)\n",
    "\n",
    "    # Display detected color & type on screen\n",
    "    cv2.putText(frame, f\"Detected: {detected_color} {clothing_type}\", (20, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    cv2.imshow(\"Clothing Detection\", frame)\n",
    "\n",
    "    # Press 'q' to capture and process\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release webcam\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if detected_color != \"unknown\":\n",
    "    # 🧠 Step 1: Get outfit suggestion from Gemini\n",
    "    suggested_outfit = get_gemini_suggestion(detected_color, clothing_type)\n",
    "    print(\"\\n👗 Outfit Suggestion:\", suggested_outfit)\n",
    "\n",
    "    # 🎨 Step 2: Generate an outfit image\n",
    "    image_url = generate_outfit_image(suggested_outfit)\n",
    "    print(\"\\n🖼️ Generated Outfit Image URL:\", image_url)\n",
    "\n",
    "    # 📷 Step 3: Display the image\n",
    "    if \"http\" in image_url:\n",
    "        display_generated_image(image_url)\n",
    "    else:\n",
    "        print(\"No image available.\")\n",
    "else:\n",
    "    print(\"No valid clothing detected. Try again.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb743805-476b-4f86-8a56-1fdd025e7cb0",
   "metadata": {},
   "source": [
    "def get_gemini_suggestion(color, clothing_type):\n",
    "    import google.generativeai as genai\n",
    "\n",
    "    genai.configure(api_key=\"AIzaSyCnRvmPOboIvqZg2dINnsUvvHMFoyWruAs\")\n",
    "\n",
    "    prompt = f\"I am wearing a {color} {clothing_type}. Suggest a compatible outfit combination.\"\n",
    "    \n",
    "    model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")  # Use an available model\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    return response.text if response else \"No suggestion available.\"\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bac43b15-52dd-45d2-a807-b9660c0d0241",
   "metadata": {},
   "source": [
    "def get_gemini_suggestion(color, clothing_type):\n",
    "    prompt = f\"I am wearing a {color} {clothing_type}. Suggest a compatible outfit combination.\"\n",
    "    response = genai.GenerativeModel(\"gemini-pro\").generate_content(prompt)\n",
    "    return response.text if response else \"No suggestion available.\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f970cd7-08b8-4778-86bf-9976590d0616",
   "metadata": {},
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "start_time = time.time()\n",
    "fixed_color = None\n",
    "fixed_type = None\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "    center_region = frame[h//2-75:h//2+75, w//2-75:w//2+75]\n",
    "\n",
    "    # Detect Color\n",
    "    hsv = cv2.cvtColor(center_region, cv2.COLOR_BGR2HSV)\n",
    "    hue = hsv[hsv.shape[0]//2, hsv.shape[1]//2][0]\n",
    "    detected_color = next((color for color, (low, high) in hue_ranges.items() if low <= hue <= high), \"Unknown\")\n",
    "\n",
    "    # Detect Clothing Type\n",
    "    detected_type = predict_clothing(center_region)\n",
    "\n",
    "    # Lock Detection after 15s\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time >= 15:\n",
    "        fixed_color, fixed_type = detected_color, detected_type\n",
    "        print(f\"\\n🔒 Locked: {fixed_color} {fixed_type}\")\n",
    "        suggestion = get_gemini_suggestion(fixed_color, fixed_type)\n",
    "        print(\"\\n👗 Outfit Suggestion:\", suggestion)\n",
    "        break\n",
    "\n",
    "    # Display\n",
    "    cv2.rectangle(frame, (w//2-75, h//2-75), (w//2+75, h//2+75), (255, 255, 255), 2)\n",
    "    cv2.putText(frame, f\"Color: {detected_color}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.putText(frame, f\"Clothing: {detected_type}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.imshow(\"Fashion AI - Outfit Selector\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "581724df-9078-488e-93e8-50cdc3e8ca5d",
   "metadata": {},
   "source": [
    "pip install --upgrade google-generativeai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fef385-ca10-4be3-b658-54775cc12361",
   "metadata": {},
   "source": [
    "# Integrating Gemini to ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7e906d7-2bbb-43c0-84cb-77f282197c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to type or speak? (type/speak):  type\n",
      "Enter your text:  Give an image of red color top\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 AI is thinking...\n",
      "\n"
     ]
    },
    {
     "ename": "NotFound",
     "evalue": "404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_query:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🤖 AI is thinking...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m     gemini_response \u001b[38;5;241m=\u001b[39m ask_gemini_ai(user_query)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🧠 Gemini AI Answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgemini_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 33\u001b[0m, in \u001b[0;36mask_gemini_ai\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a question to Gemini AI and return the response.\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-pro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_content(question)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[0;32m    332\u001b[0m             request,\n\u001b[0;32m    333\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_options,\n\u001b[0;32m    334\u001b[0m         )\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 835\u001b[0m response \u001b[38;5;241m=\u001b[39m rpc(\n\u001b[0;32m    836\u001b[0m     request,\n\u001b[0;32m    837\u001b[0m     retry\u001b[38;5;241m=\u001b[39mretry,\n\u001b[0;32m    838\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    839\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    840\u001b[0m )\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    294\u001b[0m     target,\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[0;32m    296\u001b[0m     sleep_generator,\n\u001b[0;32m    297\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m    298\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[0;32m    299\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     _retry_error_helper(\n\u001b[0;32m    154\u001b[0m         exc,\n\u001b[0;32m    155\u001b[0m         deadline,\n\u001b[0;32m    156\u001b[0m         sleep,\n\u001b[0;32m    157\u001b[0m         error_list,\n\u001b[0;32m    158\u001b[0m         predicate,\n\u001b[0;32m    159\u001b[0m         on_error,\n\u001b[0;32m    160\u001b[0m         exception_factory,\n\u001b[0;32m    161\u001b[0m         timeout,\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m target()\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mNotFound\u001b[0m: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods."
     ]
    }
   ],
   "source": [
    "genai.configure(api_key=\"AIzaSyCnRvmPOboIvqZg2dINnsUvvHMFoyWruAs\") \n",
    "\n",
    "def get_user_input():\n",
    "    \"\"\"Get user input via typing or voice recognition.\"\"\"\n",
    "    choice = input(\"Do you want to type or speak? (type/speak): \").strip().lower()\n",
    "\n",
    "    if choice == \"type\":\n",
    "        user_input = input(\"Enter your text: \")\n",
    "    elif choice == \"speak\":\n",
    "        recognizer = sr.Recognizer()\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"🎤 Speak now...\")\n",
    "            recognizer.adjust_for_ambient_noise(source) \n",
    "            try:\n",
    "                audio = recognizer.listen(source, timeout=5)  \n",
    "                user_input = recognizer.recognize_google(audio)  \n",
    "                print(f\"🗣 You said: {user_input}\")\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"❌ Could not understand audio.\")\n",
    "                return None\n",
    "            except sr.RequestError:\n",
    "                print(\"❌ API request failed.\")\n",
    "                return None\n",
    "    else:\n",
    "        print(\"❌ Invalid choice. Please enter 'type' or 'speak'.\")\n",
    "        return get_user_input()  \n",
    "\n",
    "    return user_input\n",
    "\n",
    "def ask_gemini_ai(question):\n",
    "    \"\"\"Send a question to Gemini AI and return the response.\"\"\"\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "    response = model.generate_content(question)\n",
    "    return response.text\n",
    "\n",
    "user_query = get_user_input()\n",
    "\n",
    "if user_query:\n",
    "    print(\"\\n🤖 AI is thinking...\\n\")\n",
    "    gemini_response = ask_gemini_ai(user_query)\n",
    "    print(f\"🧠 Gemini AI Answer: {gemini_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a5a44-975a-4682-826f-bb85ac30493a",
   "metadata": {},
   "source": [
    "# Weather Forecasting and Trend Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d342a138-f12a-496d-9b9d-ce86fd8b346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"3f1059aa54ed48d26fad8051e476b409\" \n",
    "BASE_URL = \"http://api.openweathermap.org/data/2.5/weather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21274a13-3127-4f9d-8599-24747f844728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_input():\n",
    "    print(\"\\n📍 How do you want to enter the city?\")\n",
    "    print(\"1️⃣ Type the city name\")\n",
    "    print(\"2️⃣ Speak the city name (Voice Input)\")\n",
    "    \n",
    "    choice = input(\"\\nEnter your choice (1/2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        return input(\"\\n✏️ Enter city name: \").strip()\n",
    "    \n",
    "    elif choice == \"2\":\n",
    "        recognizer = sr.Recognizer()\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"\\n🎤 Speak now... (Say the city name)\")\n",
    "            recognizer.adjust_for_ambient_noise(source)  \n",
    "            try:\n",
    "                city = recognizer.listen(source)\n",
    "                city_name = recognizer.recognize_google(city)\n",
    "                print(f\"✅ City recognized: {city_name}\")\n",
    "                return city_name.strip()\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"\\n⚠️ Sorry, could not understand. Please try again.\")\n",
    "                return get_city_input()\n",
    "            except sr.RequestError:\n",
    "                print(\"\\n⚠️ Voice service unavailable. Please type the city name instead.\")\n",
    "                return input(\"\\n✏️ Enter city name: \").strip()\n",
    "    else:\n",
    "        print(\"\\n⚠️ Invalid choice! Please select again.\")\n",
    "        return get_city_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77489327-a3b0-48ac-ad54-11afb61f969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfit_suggestions = {\n",
    "    \"Hot & Sunny\": [\n",
    "        \"Light cotton T-shirt and shorts with sunglasses.\",\n",
    "        \"Sleeveless tank top and linen shorts.\",\n",
    "        \"Loose cotton kurta with breathable sandals.\",\n",
    "        \"Cotton polo with chino shorts.\",\n",
    "        \"Short-sleeve shirt with Bermuda shorts.\",\n",
    "        \"Tank top and joggers for a casual look.\",\n",
    "        \"Floral summer dress with flip-flops.\",\n",
    "        \"White T-shirt and khaki shorts.\",\n",
    "        \"Light sundress with a wide-brim hat.\",\n",
    "        \"Sports T-shirt with moisture-wicking fabric.\",\n",
    "    ],\n",
    "    \"Cold & Snowy\": [\n",
    "        \"Woolen sweater with thermal pants.\",\n",
    "        \"Puffer jacket with a beanie and gloves.\",\n",
    "        \"Fleece-lined hoodie with jeans.\",\n",
    "        \"Heavy coat with thermal leggings and boots.\",\n",
    "        \"Wool coat with layered scarves and gloves.\",\n",
    "        \"Down jacket with warm socks and boots.\",\n",
    "        \"Turtleneck sweater with thermal joggers.\",\n",
    "        \"Ski jacket and insulated pants.\",\n",
    "        \"Layered flannel shirt with a winter hat.\",\n",
    "        \"Long coat with knee-high boots.\",\n",
    "    ],\n",
    "    \"Rainy\": [\n",
    "        \"Waterproof jacket with rubber boots.\",\n",
    "        \"Hooded raincoat with water-resistant shoes.\",\n",
    "        \"Light trench coat with an umbrella.\",\n",
    "        \"Poncho with quick-dry pants.\",\n",
    "        \"Denim jacket with waterproof sneakers.\",\n",
    "        \"All-weather parka with ankle boots.\",\n",
    "        \"Nylon windbreaker with waterproof leggings.\",\n",
    "        \"Hooded sweatshirt with a rainproof hat.\",\n",
    "        \"Rubberized raincoat with hiking boots.\",\n",
    "        \"Light sweater with a windproof jacket.\",\n",
    "    ],\n",
    "    \"Mild & Pleasant\": [\n",
    "        \"Cotton shirt with slim-fit jeans.\",\n",
    "        \"Light cardigan with ankle-length skirt.\",\n",
    "        \"Short-sleeve button-up with chinos.\",\n",
    "        \"Flowy maxi dress with ballet flats.\",\n",
    "        \"Casual denim jacket with sneakers.\",\n",
    "        \"Hoodie with soft joggers.\",\n",
    "        \"Polo T-shirt with comfortable jeans.\",\n",
    "        \"Light blouse with high-waisted pants.\",\n",
    "        \"Half-sleeve shirt with relaxed-fit trousers.\",\n",
    "        \"Midi dress with ankle boots.\",\n",
    "    ],\n",
    "    \"Windy\": [\n",
    "        \"Windbreaker with track pants.\",\n",
    "        \"Denim jacket with sturdy boots.\",\n",
    "        \"Light hoodie with a cap.\",\n",
    "        \"Trench coat with fitted jeans.\",\n",
    "        \"Long-sleeve pullover with sneakers.\",\n",
    "        \"Fleece jacket with cargo pants.\",\n",
    "        \"Bomber jacket with a hoodie underneath.\",\n",
    "        \"Softshell jacket with a scarf.\",\n",
    "        \"Light knitwear with ankle boots.\",\n",
    "        \"Loose sweatshirt with joggers.\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6678d9d4-4567-4a44-92eb-4aff9f5b1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 Get Weather Data\n",
    "def get_weather(city):\n",
    "    params = {\"q\": city, \"appid\": API_KEY, \"units\": \"metric\"}\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        temperature = data[\"main\"][\"temp\"]\n",
    "        weather_condition = data[\"weather\"][0][\"main\"]\n",
    "        \n",
    "        print(f\"\\n🌡️ Temperature in {city}: {temperature}°C\")\n",
    "        print(f\"🌤️ Weather Condition: {weather_condition}\")\n",
    "        \n",
    "        return temperature, weather_condition\n",
    "    else:\n",
    "        print(\"\\n❌ Error fetching weather data. Please check the city name and try again.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85fa07d2-df61-41e2-9fd6-2e6dc809ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_outfit(temp, condition):\n",
    "    if condition in [\"Clear\", \"Sunny\"] and temp > 25:\n",
    "        category = \"Hot & Sunny\"\n",
    "    elif condition in [\"Snow\", \"Cold\"] or temp < 5:\n",
    "        category = \"Cold & Snowy\"\n",
    "    elif condition in [\"Rain\", \"Drizzle\", \"Thunderstorm\"]:\n",
    "        category = \"Rainy\"\n",
    "    elif condition in [\"Clouds\"] and 10 <= temp <= 25:\n",
    "        category = \"Mild & Pleasant\"\n",
    "    elif condition in [\"Wind\", \"Mist\"] or (temp > 10 and temp < 20):\n",
    "        category = \"Windy\"\n",
    "    else:\n",
    "        category = \"Mild & Pleasant\"\n",
    "    \n",
    "    outfit = random.choice(outfit_suggestions[category])\n",
    "    print(f\"\\n👗 **Suggested Outfit for {category}:** {outfit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86009c92-6d7e-4bf1-869e-b4fc4bc29a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📍 How do you want to enter the city?\n",
      "1️⃣ Type the city name\n",
      "2️⃣ Speak the city name (Voice Input)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1/2):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎤 Speak now... (Say the city name)\n",
      "\n",
      "⚠️ Sorry, could not understand. Please try again.\n",
      "\n",
      "📍 How do you want to enter the city?\n",
      "1️⃣ Type the city name\n",
      "2️⃣ Speak the city name (Voice Input)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1/2):  1\n",
      "\n",
      "✏️ Enter city name:  Chennai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌡️ Temperature in Chennai: 30.85°C\n",
      "🌤️ Weather Condition: Haze\n",
      "\n",
      "👗 **Suggested Outfit for Mild & Pleasant:** Hoodie with soft joggers.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    city = get_city_input()\n",
    "    temp, condition = get_weather(city)\n",
    "    \n",
    "    if temp is not None and condition is not None:\n",
    "        suggest_outfit(temp, condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8041cc56-d62f-4508-88ec-95924c646c8e",
   "metadata": {},
   "source": [
    "# EfficientNetB4 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a4a47d-6ac2-4284-baf0-5256938e772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f4aa2cd-25ee-46be-9e46-ef40b490c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"C:\\\\Users\\\\dpree\\\\Downloads\\\\images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e175c438-970d-4305-8e77-83c0118d4b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,  # 80-20 split\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96118f6c-f3f9-43ce-88a0-7f441100b475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35284 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb77bae6-302d-4d28-9fde-e5ac0c3e3a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8812 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f1b0827-91a3-4331-9934-fcc01ba4433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3156f32a-9a36-4b5d-ad8d-723a02a7cfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpree\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85624a76-f508-4261-aef6-e15b6cf87348",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 📉 Callbacks\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f726e-fb2b-46f3-966f-a972a4a2dd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpree\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3165s\u001b[0m 3s/step - accuracy: 0.0325 - loss: 7.2900 - val_accuracy: 0.0262 - val_loss: 3.1376 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4806s\u001b[0m 4s/step - accuracy: 0.0270 - loss: 3.1608 - val_accuracy: 0.0161 - val_loss: 3.1361 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3141s\u001b[0m 3s/step - accuracy: 0.0459 - loss: 3.1089 - val_accuracy: 0.0161 - val_loss: 3.1395 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m 730/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m28:44\u001b[0m 5s/step - accuracy: 0.0228 - loss: 3.1705"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d0f40c6-2fe4-44de-890f-90f482edb67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 Final training and validation accuracy\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "766dfcd2-d9dc-47c7-ae79-2a2ca0da0c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpree\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5760s\u001b[0m 5s/step - accuracy: 0.0233 - loss: 3.2748 - val_accuracy: 0.0143 - val_loss: 3.0564\n",
      "Epoch 2/5\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5441s\u001b[0m 5s/step - accuracy: 0.0200 - loss: 3.1398 - val_accuracy: 5.6741e-04 - val_loss: 3.2061\n",
      "Epoch 3/5\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6210s\u001b[0m 6s/step - accuracy: 0.0164 - loss: 3.2444 - val_accuracy: 0.0051 - val_loss: 3.1006\n",
      "Epoch 4/5\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5560s\u001b[0m 5s/step - accuracy: 0.0218 - loss: 3.1868 - val_accuracy: 0.0033 - val_loss: 3.2269\n",
      "Epoch 5/5\n",
      "\u001b[1m1103/1103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9388s\u001b[0m 9s/step - accuracy: 0.0176 - loss: 3.2242 - val_accuracy: 0.0377 - val_loss: 3.0872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1af211b87a0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss, val_accuracy = model.evaluate(val_generator)\n",
    "print(f\"Evaluation Accuracy on Validation Set: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80e0afa9-06d2-401d-a6a2-92c84c0592a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
